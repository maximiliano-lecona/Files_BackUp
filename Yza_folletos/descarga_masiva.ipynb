{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28679978-341d-4a6a-821e-499807400221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 14:07:30,891 - INFO - === CONFIGURACIÓN DE DESCARGA ===\n",
      "2026-02-03 14:07:30,892 - INFO - Bucket: data-bunker-prod-env\n",
      "2026-02-03 14:07:30,894 - INFO - Prefijo: images/year=2026/month=02/channel=farmaciasGDL/\n",
      "2026-02-03 14:07:30,895 - INFO - Fecha: 02-02-2026\n",
      "2026-02-03 14:07:30,895 - INFO - Directorio de salida: ./competitors/Guadalajara/folletos/\n",
      "2026-02-03 14:07:30,896 - INFO - Usando fecha tal como está: 02-02-2026\n",
      "2026-02-03 14:07:30,897 - INFO - Fecha formateada para búsqueda: 02-02-2026\n",
      "2026-02-03 14:07:30,897 - INFO - Sin filtro adicional - solo fecha\n",
      "2026-02-03 14:07:30,898 - INFO - Usando credenciales AWS por defecto\n",
      "2026-02-03 14:07:30,898 - INFO - Hilos concurrentes: 3\n",
      "2026-02-03 14:07:30,899 - INFO - ===================================\n",
      "2026-02-03 14:07:32,293 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-dermo_page_1_02-02-2026.png\n",
      "2026-02-03 14:07:32,294 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-dermo_page_2_02-02-2026.png\n",
      "2026-02-03 14:07:32,295 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-dermo_page_3_02-02-2026.png\n",
      "2026-02-03 14:07:32,296 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-dermo_page_4_02-02-2026.png\n",
      "2026-02-03 14:07:32,296 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_10_02-02-2026.png\n",
      "2026-02-03 14:07:32,297 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_11_02-02-2026.png\n",
      "2026-02-03 14:07:32,298 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_12_02-02-2026.png\n",
      "2026-02-03 14:07:32,298 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_13_02-02-2026.png\n",
      "2026-02-03 14:07:32,299 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_14_02-02-2026.png\n",
      "2026-02-03 14:07:32,300 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_15_02-02-2026.png\n",
      "2026-02-03 14:07:32,300 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_16_02-02-2026.png\n",
      "2026-02-03 14:07:32,301 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_1_02-02-2026.png\n",
      "2026-02-03 14:07:32,301 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_2_02-02-2026.png\n",
      "2026-02-03 14:07:32,302 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_3_02-02-2026.png\n",
      "2026-02-03 14:07:32,302 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_4_02-02-2026.png\n",
      "2026-02-03 14:07:32,303 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_5_02-02-2026.png\n",
      "2026-02-03 14:07:32,304 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_6_02-02-2026.png\n",
      "2026-02-03 14:07:32,305 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_7_02-02-2026.png\n",
      "2026-02-03 14:07:32,305 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_8_02-02-2026.png\n",
      "2026-02-03 14:07:32,306 - INFO - Encontrado: images/year=2026/month=02/channel=farmaciasGDL/boletin-ofertas_page_9_02-02-2026.png\n",
      "2026-02-03 14:07:32,307 - INFO - Total de archivos encontrados: 20\n",
      "2026-02-03 14:07:32,308 - INFO - Iniciando descarga de 20 archivos...\n",
      "2026-02-03 14:07:33,013 - INFO - Descargado: boletin-dermo_page_1_02-02-2026.png -> boletin-dermo_page_1_02-02-2026.png\n",
      "2026-02-03 14:07:33,311 - INFO - Descargado: boletin-dermo_page_4_02-02-2026.png -> boletin-dermo_page_4_02-02-2026.png\n",
      "2026-02-03 14:07:33,933 - INFO - Descargado: boletin-ofertas_page_10_02-02-2026.png -> boletin-ofertas_page_10_02-02-2026.png\n",
      "2026-02-03 14:07:34,053 - INFO - Descargado: boletin-dermo_page_2_02-02-2026.png -> boletin-dermo_page_2_02-02-2026.png\n",
      "2026-02-03 14:07:34,109 - INFO - Descargado: boletin-dermo_page_3_02-02-2026.png -> boletin-dermo_page_3_02-02-2026.png\n",
      "2026-02-03 14:07:34,184 - INFO - Descargado: boletin-ofertas_page_11_02-02-2026.png -> boletin-ofertas_page_11_02-02-2026.png\n",
      "2026-02-03 14:07:34,325 - INFO - Descargado: boletin-ofertas_page_12_02-02-2026.png -> boletin-ofertas_page_12_02-02-2026.png\n",
      "2026-02-03 14:07:34,387 - INFO - Descargado: boletin-ofertas_page_13_02-02-2026.png -> boletin-ofertas_page_13_02-02-2026.png\n",
      "2026-02-03 14:07:34,399 - INFO - Descargado: boletin-ofertas_page_14_02-02-2026.png -> boletin-ofertas_page_14_02-02-2026.png\n",
      "2026-02-03 14:07:34,592 - INFO - Descargado: boletin-ofertas_page_15_02-02-2026.png -> boletin-ofertas_page_15_02-02-2026.png\n",
      "2026-02-03 14:07:34,618 - INFO - Descargado: boletin-ofertas_page_16_02-02-2026.png -> boletin-ofertas_page_16_02-02-2026.png\n",
      "2026-02-03 14:07:34,633 - INFO - Descargado: boletin-ofertas_page_1_02-02-2026.png -> boletin-ofertas_page_1_02-02-2026.png\n",
      "2026-02-03 14:07:34,818 - INFO - Descargado: boletin-ofertas_page_2_02-02-2026.png -> boletin-ofertas_page_2_02-02-2026.png\n",
      "2026-02-03 14:07:34,853 - INFO - Descargado: boletin-ofertas_page_4_02-02-2026.png -> boletin-ofertas_page_4_02-02-2026.png\n",
      "2026-02-03 14:07:34,870 - INFO - Descargado: boletin-ofertas_page_3_02-02-2026.png -> boletin-ofertas_page_3_02-02-2026.png\n",
      "2026-02-03 14:07:35,054 - INFO - Descargado: boletin-ofertas_page_5_02-02-2026.png -> boletin-ofertas_page_5_02-02-2026.png\n",
      "2026-02-03 14:07:35,072 - INFO - Descargado: boletin-ofertas_page_6_02-02-2026.png -> boletin-ofertas_page_6_02-02-2026.png\n",
      "2026-02-03 14:07:35,107 - INFO - Descargado: boletin-ofertas_page_7_02-02-2026.png -> boletin-ofertas_page_7_02-02-2026.png\n",
      "2026-02-03 14:07:35,270 - INFO - Descargado: boletin-ofertas_page_8_02-02-2026.png -> boletin-ofertas_page_8_02-02-2026.png\n",
      "2026-02-03 14:07:35,299 - INFO - Descargado: boletin-ofertas_page_9_02-02-2026.png -> boletin-ofertas_page_9_02-02-2026.png\n",
      "2026-02-03 14:07:35,300 - INFO - === DESCARGA COMPLETADA ===\n",
      "2026-02-03 14:07:35,302 - INFO -   - Exitosas: 20\n",
      "2026-02-03 14:07:35,303 - INFO -   - Fallidas: 0\n",
      "2026-02-03 14:07:35,304 - INFO -   - Total: 20\n",
      "2026-02-03 14:07:35,305 - INFO -   - Archivos guardados en: ./competitors/Guadalajara/folletos/\n",
      "2026-02-03 14:07:35,306 - INFO - ¡Proceso completado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script para descargar masivamente archivos de S3 que contengan una fecha específica \n",
    "y opcionalmente otra substring en el nombre.\n",
    "VERSIÓN CORREGIDA - Maneja caracteres especiales en nombres de archivos\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# ===========================\n",
    "# CONFIGURACIÓN - MODIFICA ESTOS VALORES\n",
    "# ===========================\n",
    "\n",
    "canal = \"farmaciasGDL\"\n",
    "carpeta_canal = \"Guadalajara\"\n",
    "fecha = \"02-02-2026\"\n",
    "\n",
    "# Configuración básica\n",
    "BUCKET_NAME = \"data-bunker-prod-env\"                    # Nombre del bucket de S3\n",
    "PREFIX = f\"images/year=2026/month=02/channel={canal}/\"                 # Prefijo dentro del bucket (incluye \"/\" al final si es carpeta)\n",
    "DATE_TO_SEARCH = f\"{fecha}\"              # Fecha a buscar (YYYY-MM-DD, YYYYMMDD, etc.)\n",
    "OUTPUT_DIR = f\"./competitors/{carpeta_canal}/folletos/\"                 # Directorio de salida para los archivos\n",
    "ADDITIONAL_FILTER = \"\" # Filtro adicional opcional (vacío = sin filtro)\n",
    "# \"Torreón\" \"Monterrey\" \"Reynosa\"\n",
    "\n",
    "# Configuración avanzada\n",
    "AWS_PROFILE = None                          # Perfil de AWS a usar (None = default)\n",
    "MAX_WORKERS = 3                             # Número de hilos para descarga paralela (reducido)\n",
    "PRESERVE_STRUCTURE = True                   # Si mantener la estructura de carpetas de S3\n",
    "VERBOSE_LOGGING = False                     # Habilitar logging detallado (cambiado a False)\n",
    "\n",
    "# ===========================\n",
    "# FUNCIONES AUXILIARES\n",
    "# ===========================\n",
    "\n",
    "def clean_filename(filename):\n",
    "    \"\"\"\n",
    "    Limpia nombres de archivo eliminando o reemplazando caracteres problemáticos\n",
    "    \"\"\"\n",
    "    # Normalizar unicode y remover acentos\n",
    "    filename = unicodedata.normalize('NFKD', filename)\n",
    "    filename = filename.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Reemplazar caracteres problemáticos\n",
    "    filename = re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "    \n",
    "    # Remover espacios múltiples y al inicio/final\n",
    "    filename = re.sub(r'\\s+', ' ', filename).strip()\n",
    "    \n",
    "    return filename\n",
    "\n",
    "# ===========================\n",
    "# CÓDIGO DEL SCRIPT\n",
    "# ===========================\n",
    "\n",
    "# Configurar logging\n",
    "log_level = logging.DEBUG if VERBOSE_LOGGING else logging.INFO\n",
    "logging.basicConfig(\n",
    "    level=log_level,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class S3BulkDownloader:\n",
    "    def __init__(self, bucket_name, aws_profile=None):\n",
    "        \"\"\"\n",
    "        Inicializa el downloader de S3\n",
    "        \n",
    "        Args:\n",
    "            bucket_name (str): Nombre del bucket de S3\n",
    "            aws_profile (str): Perfil de AWS a usar (opcional)\n",
    "        \"\"\"\n",
    "        self.bucket_name = bucket_name\n",
    "        aws_access_key_id = \"AKIARTBJEQQ4RJUEYEUN\"\n",
    "        aws_secret_access_key = \"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\"\n",
    "        aws_default_region = \"us-east-2\"\n",
    "        \n",
    "        # Crear sesión de boto3\n",
    "        if aws_profile:\n",
    "            session = boto3.Session(profile_name=aws_profile)\n",
    "            self.s3_client = session.client('s3')\n",
    "        else:\n",
    "            self.s3_client = boto3.client(\n",
    "                                's3',\n",
    "                                aws_access_key_id=aws_access_key_id,\n",
    "                                aws_secret_access_key= aws_secret_access_key,\n",
    "                                region_name= aws_default_region\n",
    "                            )\n",
    "    \n",
    "    def list_files_with_date(self, prefix, date_string, additional_filter=None):\n",
    "        \"\"\"\n",
    "        Lista archivos en S3 que contengan la fecha y opcionalmente otra substring en el nombre\n",
    "        \n",
    "        Args:\n",
    "            prefix (str): Prefijo en S3 para buscar\n",
    "            date_string (str): Fecha a buscar en el nombre del archivo\n",
    "            additional_filter (str): Substring adicional que debe contener el archivo (opcional)\n",
    "            \n",
    "        Returns:\n",
    "            list: Lista de objetos S3 que coinciden\n",
    "        \"\"\"\n",
    "        matching_files = []\n",
    "        \n",
    "        try:\n",
    "            paginator = self.s3_client.get_paginator('list_objects_v2')\n",
    "            pages = paginator.paginate(Bucket=self.bucket_name, Prefix=prefix)\n",
    "            \n",
    "            for page in pages:\n",
    "                if 'Contents' in page:\n",
    "                    for obj in page['Contents']:\n",
    "                        filename = obj['Key']\n",
    "                        \n",
    "                        # Verificar si la fecha está en el nombre del archivo\n",
    "                        has_date = date_string in filename\n",
    "                        \n",
    "                        # Verificar filtro adicional si se proporciona\n",
    "                        has_additional_filter = True\n",
    "                        if additional_filter and additional_filter.strip():\n",
    "                            has_additional_filter = additional_filter in filename\n",
    "                        \n",
    "                        # El archivo debe cumplir ambas condiciones\n",
    "                        if has_date and has_additional_filter:\n",
    "                            matching_files.append(obj)\n",
    "                            logger.info(f\"Encontrado: {filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error listando archivos: {e}\")\n",
    "            return []\n",
    "        \n",
    "        logger.info(f\"Total de archivos encontrados: {len(matching_files)}\")\n",
    "        return matching_files\n",
    "    \n",
    "    def download_file(self, s3_key, local_path):\n",
    "        \"\"\"\n",
    "        Descarga un archivo individual de S3\n",
    "        \n",
    "        Args:\n",
    "            s3_key (str): Key del objeto en S3\n",
    "            local_path (str): Ruta local donde guardar el archivo\n",
    "            \n",
    "        Returns:\n",
    "            bool: True si la descarga fue exitosa\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Crear directorios padre si no existen\n",
    "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "            \n",
    "            # Descargar archivo\n",
    "            self.s3_client.download_file(self.bucket_name, s3_key, local_path)\n",
    "            logger.info(f\"Descargado: {os.path.basename(s3_key)} -> {os.path.basename(local_path)}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error descargando {s3_key}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def bulk_download(self, prefix, date_string, output_dir, additional_filter=None, max_workers=3, preserve_structure=True):\n",
    "        \"\"\"\n",
    "        Descarga masivamente archivos que contengan la fecha y opcionalmente otra substring\n",
    "        \n",
    "        Args:\n",
    "            prefix (str): Prefijo en S3 para buscar\n",
    "            date_string (str): Fecha a buscar en nombres de archivo\n",
    "            output_dir (str): Directorio de salida\n",
    "            additional_filter (str): Substring adicional que debe contener el archivo (opcional)\n",
    "            max_workers (int): Número máximo de hilos concurrentes\n",
    "            preserve_structure (bool): Si mantener la estructura de carpetas de S3\n",
    "        \"\"\"\n",
    "        # Crear directorio de salida si no existe\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Buscar archivos que coincidan\n",
    "        matching_files = self.list_files_with_date(prefix, date_string, additional_filter)\n",
    "        \n",
    "        if not matching_files:\n",
    "            logger.warning(\"No se encontraron archivos que coincidan con los criterios\")\n",
    "            return\n",
    "        \n",
    "        # Preparar lista de descargas\n",
    "        download_tasks = []\n",
    "        for obj in matching_files:\n",
    "            s3_key = obj['Key']\n",
    "            \n",
    "            if preserve_structure:\n",
    "                # Mantener estructura de carpetas\n",
    "                relative_path = s3_key[len(prefix):].lstrip('/')\n",
    "                # LIMPIAR EL NOMBRE DEL ARCHIVO\n",
    "                relative_path = clean_filename(relative_path)\n",
    "                local_path = os.path.join(output_dir, relative_path)\n",
    "            else:\n",
    "                # Todos los archivos en el directorio raíz\n",
    "                filename = os.path.basename(s3_key)\n",
    "                # LIMPIAR EL NOMBRE DEL ARCHIVO\n",
    "                filename = clean_filename(filename)\n",
    "                local_path = os.path.join(output_dir, filename)\n",
    "            \n",
    "            download_tasks.append((s3_key, local_path))\n",
    "        \n",
    "        # Ejecutar descargas en paralelo\n",
    "        successful_downloads = 0\n",
    "        failed_downloads = 0\n",
    "        \n",
    "        logger.info(f\"Iniciando descarga de {len(download_tasks)} archivos...\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Enviar todas las tareas\n",
    "            future_to_download = {\n",
    "                executor.submit(self.download_file, s3_key, local_path): (s3_key, local_path)\n",
    "                for s3_key, local_path in download_tasks\n",
    "            }\n",
    "            \n",
    "            # Procesar resultados conforme se completan\n",
    "            for future in as_completed(future_to_download):\n",
    "                s3_key, local_path = future_to_download[future]\n",
    "                try:\n",
    "                    success = future.result()\n",
    "                    if success:\n",
    "                        successful_downloads += 1\n",
    "                    else:\n",
    "                        failed_downloads += 1\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error en descarga de {s3_key}: {e}\")\n",
    "                    failed_downloads += 1\n",
    "        \n",
    "        # Resumen final\n",
    "        logger.info(f\"=== DESCARGA COMPLETADA ===\")\n",
    "        logger.info(f\"  - Exitosas: {successful_downloads}\")\n",
    "        logger.info(f\"  - Fallidas: {failed_downloads}\")\n",
    "        logger.info(f\"  - Total: {len(download_tasks)}\")\n",
    "        logger.info(f\"  - Archivos guardados en: {output_dir}\")\n",
    "\n",
    "def parse_date(date_string):\n",
    "    \"\"\"\n",
    "    Valida y formatea la fecha de entrada\n",
    "    \n",
    "    Args:\n",
    "        date_string (str): Fecha en formato YYYY-MM-DD o YYYYMMDD\n",
    "        \n",
    "    Returns:\n",
    "        str: Fecha formateada para búsqueda\n",
    "    \"\"\"\n",
    "    # Si la fecha contiene guiones o barras, usarla tal como está\n",
    "    # Esto es útil cuando queremos buscar el formato exacto del archivo\n",
    "    if '-' in date_string or '/' in date_string:\n",
    "        logger.info(f\"Usando fecha tal como está: {date_string}\")\n",
    "        return date_string\n",
    "    \n",
    "    # Intentar diferentes formatos solo si es numérico\n",
    "    formats = ['%Y%m%d', '%d%m%Y']\n",
    "    \n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_string, fmt)\n",
    "            # Devolver en formato YYYYMMDD para búsqueda\n",
    "            return date_obj.strftime('%Y%m%d')\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # Si no se puede parsear, usar como está\n",
    "    logger.warning(f\"No se pudo parsear la fecha '{date_string}', usando como está\")\n",
    "    return date_string\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta la descarga masiva\n",
    "    \"\"\"\n",
    "    # Mostrar configuración\n",
    "    logger.info(\"=== CONFIGURACIÓN DE DESCARGA ===\")\n",
    "    logger.info(f\"Bucket: {BUCKET_NAME}\")\n",
    "    logger.info(f\"Prefijo: {PREFIX}\")\n",
    "    logger.info(f\"Fecha: {DATE_TO_SEARCH}\")\n",
    "    logger.info(f\"Directorio de salida: {OUTPUT_DIR}\")\n",
    "    \n",
    "    # Formatear fecha para búsqueda\n",
    "    search_date = parse_date(DATE_TO_SEARCH)\n",
    "    logger.info(f\"Fecha formateada para búsqueda: {search_date}\")\n",
    "    \n",
    "    # Mostrar filtros aplicados\n",
    "    if ADDITIONAL_FILTER and ADDITIONAL_FILTER.strip():\n",
    "        logger.info(f\"Filtro adicional: '{ADDITIONAL_FILTER}'\")\n",
    "    else:\n",
    "        logger.info(\"Sin filtro adicional - solo fecha\")\n",
    "    \n",
    "    if AWS_PROFILE:\n",
    "        logger.info(f\"Perfil AWS: {AWS_PROFILE}\")\n",
    "    else:\n",
    "        logger.info(\"Usando credenciales AWS por defecto\")\n",
    "    \n",
    "    logger.info(f\"Hilos concurrentes: {MAX_WORKERS}\")\n",
    "    logger.info(\"=\" * 35)\n",
    "    \n",
    "    try:\n",
    "        # Inicializar downloader\n",
    "        downloader = S3BulkDownloader(BUCKET_NAME, AWS_PROFILE)\n",
    "        \n",
    "        # Ejecutar descarga masiva\n",
    "        downloader.bulk_download(\n",
    "            prefix=PREFIX,\n",
    "            date_string=search_date,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            additional_filter=ADDITIONAL_FILTER,\n",
    "            max_workers=MAX_WORKERS,\n",
    "            preserve_structure=PRESERVE_STRUCTURE\n",
    "        )\n",
    "        \n",
    "        logger.info(\"¡Proceso completado exitosamente!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error durante la ejecución: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8b747-785e-4c6e-8181-fb22a3a68829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
